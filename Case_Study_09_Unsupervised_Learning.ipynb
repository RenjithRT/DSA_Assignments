{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Case Study #09- Unsupervised Learning.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "141-Q8NRzH81wUPVv0-ST3mEwrR_4xfpA",
      "authorship_tag": "ABX9TyP48JOZFE356/FsdtWnuoCd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RenjithRT/DSA_Assignments/blob/main/Case_Study_09_Unsupervised_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Case study on Unsupervised Learning\n",
        "\n",
        "Do the following in the wine dataset.\n",
        "1. Read the dataset to the python environment.\n",
        "2. Try out different clustering models in the wine dataset.\n",
        "3. Find the optimum number of clusters in each model and create the model with\n",
        "the optimum number of clusters."
      ],
      "metadata": {
        "id": "MjKbqaagxOxR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNAFYJbaxOAB"
      },
      "outputs": [],
      "source": [
        "# Import the required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Read the dataset to the python environment."
      ],
      "metadata": {
        "id": "ZrQQwCEqxoBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset into the dataframe\n",
        "# Read the wine dataset\n",
        "wine_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Week 14/Case Study/Wine.csv') "
      ],
      "metadata": {
        "id": "XPUt4s3jxkJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the data\n",
        "wine_df.head()"
      ],
      "metadata": {
        "id": "01Du9_v4xkO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of elements in each dimension (Rows and Columns)\n",
        "wine_df.shape"
      ],
      "metadata": {
        "id": "n8GGnu0nxkSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary of the data\n",
        "wine_df.info()"
      ],
      "metadata": {
        "id": "ECATKwB1EpUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the columns in the dataset\n",
        "wine_df.columns"
      ],
      "metadata": {
        "id": "4Wn9kp8qFXgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the null values present in each columns in the dataset \n",
        "wine_df.isna().sum()"
      ],
      "metadata": {
        "id": "6nCMSqWJxkZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The Statistical summary of wine dataset\n",
        "wine_df.describe().T"
      ],
      "metadata": {
        "id": "b4HSwnB8xkcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Try out different clustering models in the wine dataset.\n",
        "\n",
        "We can go for two clustering models.\n",
        "1. K-Means Clustering\n",
        "2. Hierarchial Clustering - Agglomerative Clustering"
      ],
      "metadata": {
        "id": "oFzH5pAtG2ni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Find the optimum number of clusters in each model and create the model with the optimum number of clusters."
      ],
      "metadata": {
        "id": "YoasRnSgihqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. K-Means Clustering**"
      ],
      "metadata": {
        "id": "NRE9Ms1wincv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the data\n",
        "from sklearn.preprocessing import Normalizer\n",
        "data = pd.DataFrame(Normalizer().fit_transform(wine_df), columns=wine_df.columns)\n",
        "data.describe().T"
      ],
      "metadata": {
        "id": "eAQ8W6fbxkgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, some feature values differ from others multiple times. The features with higher values will dominate the leaning process. However, it does not mean those variables are more important to predict the outcome of the model. Data normalization transforms multiscaled data to the same scale. **After normalization, all variables have a similar influence on the model, improving the stability and performance of the learning algorithm.**"
      ],
      "metadata": {
        "id": "dwuB-Mv5JGJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. K-Means Clustering**"
      ],
      "metadata": {
        "id": "J24bgkmPJMF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the optimal clusters using Elbow diagram\n",
        "from sklearn.cluster import KMeans\n",
        "# Assign a range for optimal cluster points\n",
        "ks = range(1,10)\n",
        "# Create an empty list for getting the inertia values\n",
        "inertia = [] \n",
        "# Creating an instace of the model using for loop\n",
        "for k in ks:\n",
        "    # fit the model with the data and check the inertia values to the empty list\n",
        "    inertia.append(KMeans(n_clusters=k, init = \"k-means++\", random_state=42).fit(data).inertia_)\n",
        "\n",
        "# Assign the figure size\n",
        "plt.figure(figsize = (16, 8))\n",
        "# Plotting the number of clusters and inertia\n",
        "plt.plot(ks, inertia, \"-o\")\n",
        "# Title of the graph\n",
        "plt.title(\"Number of Clusters vs Distance\")\n",
        "# Graph X axis's label name\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "# Graph Y axis's label name\n",
        "plt.ylabel(\"Distance\")\n",
        "# show the graph\n",
        "plt.show()\n",
        "# Check the interia \n",
        "inertia"
      ],
      "metadata": {
        "id": "4KqLUQ3nxkjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above elbow plot it is clear that, the number of clusters from 5 onwards the inertia value decreases slowly. Therefore we can select the optimun value as 5."
      ],
      "metadata": {
        "id": "ODO0y2pyMNsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inertia measures how well a dataset was clustered by K-Means. It is calculated by measuring the distance between each data point and its centroid, squaring this distance, and summing these squares across one cluster. A good model is one with low inertia AND a low number of clusters ( K )."
      ],
      "metadata": {
        "id": "W8EvSu_8MKyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can assign the optimal value as 3 to the KMeans model\n",
        "model_kmeans = KMeans(n_clusters=5, init = \"k-means++\", random_state=42)\n",
        "model_kmeans.fit(data)\n",
        "kmean_clusters = model_kmeans.labels_\n",
        "kmean_clusters"
      ],
      "metadata": {
        "id": "itUNROSkxkqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy the dataframe to the another dataframe\n",
        "new_data=data.copy()\n",
        "# Assigning labels to the target variable named Wine_Classes_Kmeans\n",
        "new_data['Wine_Classes_Kmeans']=kmean_clusters              \n",
        "new_data.head(5)"
      ],
      "metadata": {
        "id": "ZpwfFG6oxktq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the digram using sactter plot with Alcohol and Malic Acid\n",
        "plt.title(\"Alcohol and Malic_Acid\")\n",
        "plt.scatter(data[\"Alcohol\"], data[\"Malic_Acid\"], c = kmean_clusters)\n",
        "plt.xlabel(\"Alchol\")\n",
        "plt.ylabel(\"Malic_Acid\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xscd5dRUxkw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above scatter plot , it is well clear that the two features of Alcohol and Malic Acid are visualized five optimum clusters, since we have optimised the cluster value as 5 and each data points separated with 5 distinct colors."
      ],
      "metadata": {
        "id": "TsOjH07ORZmO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Hierarchial Clustering - Agglomerative Clustering**"
      ],
      "metadata": {
        "id": "D8RcGr62SMiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the dendrogram to find the number of clusters by ward method\n",
        "from scipy.cluster.hierarchy import dendrogram,linkage\n",
        "linked=linkage(data,method='ward')\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.title(\"Dendrogram\")\n",
        "plt.hlines(0.2, 0, 5000, linestyles=\"dashed\")\n",
        "dendrogram(linked,orientation='top',distance_sort='ascending',show_leaf_counts=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nK8O68Yrxk0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above dendrogram plot, it is well clear that, the number of clusters using horizontal line we can identifies that the optimun cluster value as 5. "
      ],
      "metadata": {
        "id": "xU5UIBN5hrmK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can assign the optimum value to the Agglomerative clustering model."
      ],
      "metadata": {
        "id": "6KTyCur-iO73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agglomerative clustering\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "model_agglo = AgglomerativeClustering(n_clusters= 5, affinity= \"euclidean\", linkage = \"ward\").fit(data)\n",
        "agglo_clusters = model_agglo.labels_\n",
        "agglo_clusters"
      ],
      "metadata": {
        "id": "0OSPqbt3xk3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assigning labels to the target variable named Wine_Classes_Agglomerative\n",
        "new_data['Wine_Classes_Agglomerative']=agglo_clusters              \n",
        "new_data.head(5)"
      ],
      "metadata": {
        "id": "FYKzz5Dlxk7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the digram using sactter plot with Alcohol and Malic Acid\n",
        "plt.title(\"Alcohol and Malic_Acid\")\n",
        "plt.scatter(data[\"Alcohol\"], data[\"Malic_Acid\"], c = agglo_clusters)\n",
        "plt.xlabel(\"Alchol\")\n",
        "plt.ylabel(\"Malic_Acid\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UQ1zA84Sxk-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above scatter plot , it is well clear that the two features of Alcohol and Malic Acid are visualized five optimum clusters, since we have optimised the cluster value as 5 and each data points separated with 5 distinct colors."
      ],
      "metadata": {
        "id": "lVabVZe9g__c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above two clustering models (K-Means Clustering, Hierarchial Clustering - Agglomerative Clustering) we can clearly say that the wine dataset can be divided into 5 clusters."
      ],
      "metadata": {
        "id": "xOz8ot7gs41A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning problems, there are often too many factors on the basis of which the final classification is done. These factors are basically variables called features. The higher the number of features, the harder it gets to visualize the training set and then work on it. Sometimes, most of these features are correlated, and hence redundant. This is where dimensionality reduction algorithms come into play. Dimensionality reduction is the process of reducing the number of random variables under consideration, by obtaining a set of principal variables. It can be divided into feature selection and feature extraction."
      ],
      "metadata": {
        "id": "0l4YyFgTsvsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can go for the method is Principal Components Analysis - It combine multiple, coorelated variables into a single components"
      ],
      "metadata": {
        "id": "mrxDGYjIuRTr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PCA (Principal Components Analysis)**"
      ],
      "metadata": {
        "id": "iqC7dhNDum2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# number of elements in each dimension (Rows and Columns)\n",
        "wine_df.shape"
      ],
      "metadata": {
        "id": "mpRDouNIuQoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scaling the data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scalar = StandardScaler()\n",
        "scaled_data = scalar.fit_transform(wine_df)\n",
        "scaled_data = pd.DataFrame(scaled_data, columns = data.columns)\n",
        "scaled_data.describe()"
      ],
      "metadata": {
        "id": "-jHdklR8sylV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Doing PCA\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components= 0.95)\n",
        "pca.fit(scaled_data)\n",
        "data_pca = pca.transform(scaled_data)\n",
        "print(f'Number of Principal Components to explain 95% variance = {pca.n_components_}')"
      ],
      "metadata": {
        "id": "4rGqEG5Jwlmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the graph of Components vs Explained Variance\n",
        "plt.figure(figsize = (12, 8))\n",
        "x = np.arange(1, pca.n_components_+1 , step = 1)\n",
        "y = np.cumsum(pca.explained_variance_ratio_)\n",
        "plt.plot(x, y, marker = \"o\", linestyle = \"--\", color = \"b\")\n",
        "# horizontal line for 95% cutoff threshold\n",
        "plt.axhline(y = 0.95, color = 'r', linestyle = \"-\")\n",
        "plt.text(1.2, 0.93, \"95% cut-off threshold\", color = 'b', fontsize = 14)\n",
        "plt.xticks(x)\n",
        "plt.grid(axis = 'x')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Variance (%)')\n",
        "plt.title(\"Components vs Explained Variance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Yksa68LowuzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explained variance represents the information explained using a particular principal components (eigenvectors) and it is calculated as ratio of eigenvalue of a articular principal component (eigenvector) with total eigenvalues. \n",
        "\n",
        "From the above graph, the Cumulative variance plot clearly shows the contribution of each principal components."
      ],
      "metadata": {
        "id": "VYFkb_7IxsI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca.explained_variance_ratio_"
      ],
      "metadata": {
        "id": "_S6NW136xdLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As per the PCA explained variance ratios, we can clearly says that, there are 10 components where:\n",
        "- The first principal component explains 36.19% of the total variations in the dataset.\n",
        "- The second principal component explains 19.20% of the total variations.\n",
        "- The third principal component explains 11.12% of the total variations and so on. "
      ],
      "metadata": {
        "id": "rSJ3YjLhxizW"
      }
    }
  ]
}
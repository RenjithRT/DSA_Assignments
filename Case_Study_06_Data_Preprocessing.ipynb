{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Case Study #06 -Data Preprocessing.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Case study on Pre-processing\n",
        "### Do the following on titanic dataset.\n",
        "1. Load the dataset into python environment\n",
        "2. Make ‘PassengerId’ as the index column\n",
        "3. Check the basic details of the dataset\n",
        "4. Fill in all the missing values present in all the columns in the dataset\n",
        "5. Check and handle outliers in at least 3 columns in the dataset\n",
        "6. Do min max scaling on the feature set (Take ‘Survived’ as target"
      ],
      "metadata": {
        "id": "Xs5P1eNCqSsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "g2u9df3sqbq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Load the dataset into python environment"
      ],
      "metadata": {
        "id": "uvpPLG3VsJ0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the csv file into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/titanic_dataset.csv\")"
      ],
      "metadata": {
        "id": "e80imUj8qcBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display the contents in the dataset with 3 rows\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "jqWvvhpMtxuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Make ‘PassengerId’ as the index column"
      ],
      "metadata": {
        "id": "Vg_IsyZfsX_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Before set index, check the columns in the dataset\n",
        "df.columns"
      ],
      "metadata": {
        "id": "thLKNYDPtYhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset the index as ‘PassengerId’ feature\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/titanic_dataset.csv\",index_col='PassengerId') # or we can use  df.set_index('PassengerId') "
      ],
      "metadata": {
        "id": "4b9nHR3KqcdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After set index, check the columns in the dataset\n",
        "df.columns"
      ],
      "metadata": {
        "id": "H1lIZbDKwhc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display the contents in the dataset with 3 rows\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "w0XrxMiruAIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Check the basic details of the dataset"
      ],
      "metadata": {
        "id": "ePwatRgyspSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# display the contents in the dataset\n",
        "df"
      ],
      "metadata": {
        "id": "kU4fSKmxsrO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The summary of the data\n",
        "df.info()"
      ],
      "metadata": {
        "id": "mCBkftoow7Jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of elements in each dimension (Rows and Columns)\n",
        "df.shape"
      ],
      "metadata": {
        "id": "o6DZEwKmxET0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the columns in the dataset\n",
        "df.columns"
      ],
      "metadata": {
        "id": "A2-l6qdFxM6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our titanic dataset has 891 rows with 11 features, it includes Survived, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin and Embarked. As per the dataset has some missing values. we can treat missing values after the basic questions."
      ],
      "metadata": {
        "id": "VFx7IHwtxREn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Fill in all the missing values present in all the columns in the dataset"
      ],
      "metadata": {
        "id": "YJLKxysfsriu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the null values present in each columns in the dataset\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "BrgdZd4ysvmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# length of the dataset\n",
        "len(df)"
      ],
      "metadata": {
        "id": "HWmGm9ol2K4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of elements in each dimension (Rows and Columns)\n",
        "df.shape"
      ],
      "metadata": {
        "id": "GwFsMfAcBpqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to determine the relative frequency of the missings per column to decide whether to simply drop or substitute the missings\n",
        "df.isna().sum()/(len(df))*100"
      ],
      "metadata": {
        "id": "KI0AuPyL1_AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see that there are null values present in the dataset. The features age, cabin and embarked are having the null values. we can treat the null values. \n",
        "\n",
        "cabin feature have 77% out of 100% null values present in the dataset so we can remove the cabin. If the columns have more than half of the rows as null values then the entire column can be dropped. Hence it's better to drop the column for a good output. But in our question clearly mentioned that fill the all missing values. Since cabin is a categorical data we can fill the missing values with mode method.\n",
        "\n",
        "Age and embarked have 19.87% and 0.22% null values , we can treat the age and embarked features."
      ],
      "metadata": {
        "id": "JOq-xhE-2qM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the data before missing values treatment\n",
        "df[df.isna().any(axis=1)] # check at least one null values in a row "
      ],
      "metadata": {
        "id": "FmYnGVzyzGB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the null values present in each columns in the dataset\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "aSOqwLkgEYvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can treat the missing values for age feature"
      ],
      "metadata": {
        "id": "5r842jDSSMD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The summary of the data\n",
        "df.info()"
      ],
      "metadata": {
        "id": "-WO8icr_FyYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can plot the frequency graph\n",
        "freqgraph = df.select_dtypes(include=['float','int64'])\n",
        "freqgraph.hist(figsize=(15,10))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m9XmmqtHFonl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above frequency graph we can say that age is not completely normally distributed or we can say that it is not completely right skewed data. Hence we can use median method to handle the missing values in the age feature."
      ],
      "metadata": {
        "id": "_O4MkIF5KQmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# filling missing values for age we can use median method\n",
        "df['Age'] = df['Age'].fillna(df['Age'].median()) # or df['Age'] = df['Age'].replace(np.NaN,df['Age'].median())"
      ],
      "metadata": {
        "id": "OBIOgs7ILGtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the null values treated or not for age feature\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "MCsLpXd5LHD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the data type for all features in the dataset\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "morukEPnSi9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see it is treated well for missing values in age feature. And now, we can fill the missing values for Cabin and Embarked, it is an object data types so we can use mode function to treat the missing values."
      ],
      "metadata": {
        "id": "HTra2jmfSXLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the Cabin feature\n",
        "df.Cabin.value_counts()"
      ],
      "metadata": {
        "id": "LZtwFqL1RoPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for mode for Cabin feature\n",
        "df['Cabin'].mode()"
      ],
      "metadata": {
        "id": "ufZoQHJJR96w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that it is a multimodal data. So we can fill the missing values with any of these modes. we can fill the missing values with G6"
      ],
      "metadata": {
        "id": "pTJG_QhhUAiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill the missing values for Cabin\n",
        "df['Cabin']= df['Cabin'].fillna('G6')"
      ],
      "metadata": {
        "id": "tij8EBqoT4_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the null values treated or not for Cabin\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "hxLN0EPYT5O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding null values in the column 'Embarked'\n",
        "df[df['Embarked'].isnull()]"
      ],
      "metadata": {
        "id": "2damQ3qpVEAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating the count of Embarked values in the dataset\n",
        "df.Embarked.value_counts()"
      ],
      "metadata": {
        "id": "t9TFsewvLHKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embarked implies where the traveler mounted from. There are three possible values for Embark — Southampton, Cherbourg, and Queenstown."
      ],
      "metadata": {
        "id": "6iwzheKbPrbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating the count of Embarked values in the dataset\n",
        "df1 = df.Embarked.value_counts()/(len(df))*100\n",
        "df1"
      ],
      "metadata": {
        "id": "gpgBssFFLHQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.plot(kind='bar', figsize=(7, 6), rot=0)\n",
        "plt.xlabel(\"Embarked\", labelpad=14)\n",
        "plt.ylabel(\"Count of travels\", labelpad=14)\n",
        "plt.title(\"Count of travelers by Embarked\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3wXfHEE2LHW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "More than 70% of the people boarded from Southampton. Just under 20% boarded from Cherbourg and the rest boarded from Queenstown. People who boarded from Cherbourg had a higher chance of survival than people who boarded from Southampton or Queenstown."
      ],
      "metadata": {
        "id": "Q99mcvywnOlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Southampton (S) is repeated 644 times in embarked so we can use mode value \"S\" to fill the null values in the embarked feature\n",
        "# fill the missing values for embarked\n",
        "df['Embarked'] = df['Embarked'].fillna('S')"
      ],
      "metadata": {
        "id": "xSsc-z2dnQwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the null values treated or not \n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "0esKJch8ouf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the data after missing values treatment\n",
        "df[df.isna().any(axis=1)] # check at least one null values in a row "
      ],
      "metadata": {
        "id": "mmf40Cico3Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our dataset is completely treated with no null values"
      ],
      "metadata": {
        "id": "4gi-z_JBpHlY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Check and handle outliers in at least 3 columns in the dataset"
      ],
      "metadata": {
        "id": "C86uk9hcsv8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# display the contents in the dataset\n",
        "df.head()"
      ],
      "metadata": {
        "id": "c_JB0nmzszjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Fare feature**"
      ],
      "metadata": {
        "id": "l8z8AZ33yTWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for finding outliers we can use boxplot\n",
        "# boxplot before removing the outliers from fare feature\n",
        "plt.boxplot(df['Fare'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "23VMziM9p8om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see that there are outliers present in the fare feature so we need to remove the outliers."
      ],
      "metadata": {
        "id": "3eJelU12qfDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for removing outliers first we need to check the quartiles. It will manage the outliers\n",
        "Q1 = np.percentile(df['Fare'],25,interpolation='midpoint')\n",
        "Q2 = np.percentile(df['Fare'],50,interpolation='midpoint')\n",
        "Q3 = np.percentile(df['Fare'],75,interpolation='midpoint')\n",
        "print('Q1: ',Q1,'\\nQ2: ',Q2,'\\nQ3: ',Q3)"
      ],
      "metadata": {
        "id": "-V-h_n_hp8_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the inter quartile range (IQR)\n",
        "IQR = Q3 - Q1\n",
        "print('IQR: ',IQR)"
      ],
      "metadata": {
        "id": "DTDng1XTp9G9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the lower and upper limit \n",
        "low_lm = Q1-1.5*IQR\n",
        "upp_lm = Q3+1.5*IQR\n",
        "print(\"Lower limit is : \",low_lm)\n",
        "print(\"Upper limit is : \",upp_lm)"
      ],
      "metadata": {
        "id": "rOdj83V-p9Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normally the datapoints which fall below Q1-1.5(IQR) and above Q3+1.5(IQR) are considered as outliers. If the value above the upper limit or below the lower limit we need to remove that outliers."
      ],
      "metadata": {
        "id": "L-HQFx5xrKIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# display the outilers\n",
        "outliers = []\n",
        "for i in df['Fare']:\n",
        "  if((i>upp_lm)or(i<low_lm)):\n",
        "    outliers.append(i)\n",
        "\n",
        "print(\"Outliers in the fare feature are : \",outliers)"
      ],
      "metadata": {
        "id": "nF0Kcq3sp9WO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see that these values are the outliers in the fare feature and also we can observe that there is no negative(lower limit) values and we have all the upper limit values as outliers. Now we need to find the index values for the outliers. "
      ],
      "metadata": {
        "id": "_aYNdSERrgEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select the index of these outliers\n",
        "indx1 = df['Fare']>upp_lm\n",
        "outlier_index = df.loc[indx1].index\n",
        "outlier_index"
      ],
      "metadata": {
        "id": "k-UahTJjp9eS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop these index for removing outliers\n",
        "df.drop(outlier_index, inplace=True)"
      ],
      "metadata": {
        "id": "rKorbLn5vLlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# boxplot after removing the outliers from fare feature\n",
        "plt.boxplot(df['Fare'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "71z4T48jxe0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above boxplot we can see that we reduced the outliers from the fare feature."
      ],
      "metadata": {
        "id": "vs2A0Dcdx21v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Age feature**"
      ],
      "metadata": {
        "id": "drLbQsk4yfH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# boxplot before removing the outliers from fare feature\n",
        "plt.boxplot(df['Age'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s9vpdtLnxfXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see that there are outliers present in the age feature so we need to remove the outliers."
      ],
      "metadata": {
        "id": "DjS-4TiXXesO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for removing outliers first we need to check the quartiles. It will manage the outliers\n",
        "Q1 = np.percentile(df['Age'],25,interpolation='midpoint')\n",
        "Q2 = np.percentile(df['Age'],50,interpolation='midpoint')\n",
        "Q3 = np.percentile(df['Age'],75,interpolation='midpoint')\n",
        "print('Q1: ',Q1,'\\nQ2: ',Q2,'\\nQ3: ',Q3)\n",
        "\n",
        "# check the inter quartile range (IQR)\n",
        "IQR = Q3 - Q1\n",
        "print('IQR: ',IQR)\n",
        "\n",
        "#check the lower and upper limit \n",
        "low_lm = Q1-1.5*IQR\n",
        "upp_lm = Q3+1.5*IQR\n",
        "print(\"Lower limit is : \",low_lm)\n",
        "print(\"Upper limit is : \",upp_lm)\n",
        "\n",
        "# display the outilers\n",
        "outliers = []\n",
        "for i in df['Age']:\n",
        "  if((i>upp_lm)or(i<low_lm)):\n",
        "    outliers.append(i)\n",
        "\n",
        "print(\"Outliers in the age feature are : \",outliers)"
      ],
      "metadata": {
        "id": "4Lv0-jz6xfdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see that these values are the outliers in the age feature and also we can observe that there are lower limit values and the upper limit values present in age column as outliers. Now we need to find the index values for the outliers."
      ],
      "metadata": {
        "id": "rwQlnszQYa6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select the index of these outliers\n",
        "indx1 = df['Fare']>upp_lm\n",
        "outlier_index = df.loc[indx1].index\n",
        "outlier_index"
      ],
      "metadata": {
        "id": "LnFxMu0wxfh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop these index for removing outliers\n",
        "df.drop(outlier_index, inplace=True)"
      ],
      "metadata": {
        "id": "JedKxbL9Zfak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# boxplot after removing the outliers from age feature\n",
        "plt.boxplot(df['Age'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fgy2G8b-ZkrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above boxplot we can see that we reduced the outliers from the age feature."
      ],
      "metadata": {
        "id": "GZDmpnKQZ1mO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. SibSp feature**"
      ],
      "metadata": {
        "id": "rYId497caDQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# boxplot before removing the outliers from SibSp feature\n",
        "plt.boxplot(df['SibSp'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SFboP4jBZk5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see that there are outliers present in the SibSp feature so we need to remove the outliers."
      ],
      "metadata": {
        "id": "OWQWx_Z5aRCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for removing outliers first we need to check the quartiles. It will manage the outliers\n",
        "Q1 = np.percentile(df['SibSp'],25,interpolation='midpoint')\n",
        "Q2 = np.percentile(df['SibSp'],50,interpolation='midpoint')\n",
        "Q3 = np.percentile(df['SibSp'],75,interpolation='midpoint')\n",
        "print('Q1: ',Q1,'\\nQ2: ',Q2,'\\nQ3: ',Q3)\n",
        "\n",
        "# check the inter quartile range (IQR)\n",
        "IQR = Q3 - Q1\n",
        "print('IQR: ',IQR)\n",
        "\n",
        "#check the lower and upper limit \n",
        "low_lm = Q1-1.5*IQR\n",
        "upp_lm = Q3+1.5*IQR\n",
        "print(\"Lower limit is : \",low_lm)\n",
        "print(\"Upper limit is : \",upp_lm)\n",
        "\n",
        "# display the outilers\n",
        "outliers = []\n",
        "for i in df['SibSp']:\n",
        "  if((i>upp_lm)or(i<low_lm)):\n",
        "    outliers.append(i)\n",
        "\n",
        "print(\"Outliers in the SibSp feature are : \",outliers)"
      ],
      "metadata": {
        "id": "mcbWTG-daN5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see that these values are the outliers in the SibSp feature and also we can observe that there is only the upper limit values present in SibSp column as outliers. Now we need to find the index values for the outliers."
      ],
      "metadata": {
        "id": "Mwp7PzlTahWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select the index of these outliers\n",
        "indx1 = df['SibSp']>upp_lm\n",
        "outlier_index = df.loc[indx1].index\n",
        "outlier_index"
      ],
      "metadata": {
        "id": "P_iqENB4aOFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop these index for removing outliers\n",
        "df.drop(outlier_index, inplace=True)"
      ],
      "metadata": {
        "id": "TRfaxu0QaOOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# boxplot after removing the outliers from SibSp feature\n",
        "plt.boxplot(df['SibSp'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6oK4yvcZasbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above boxplot we can see that we removed all the outliers from the SibSp feature."
      ],
      "metadata": {
        "id": "7GqsVflBbgMs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Do min max scaling on the feature set (Take ‘Survived’ as target)"
      ],
      "metadata": {
        "id": "4nyalhQSsz5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before moving to the scaling we need to check the Dummy encoding with target variable 'Survived'."
      ],
      "metadata": {
        "id": "amxZGWhWy_Sj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check the number of unique values in the gender\n",
        "df.Survived.nunique()"
      ],
      "metadata": {
        "id": "9L92mVQQs6zR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Survived'].value_counts()"
      ],
      "metadata": {
        "id": "2fQk1RjNzikf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Survived'].value_counts().plot(kind='bar', figsize=(7, 6), rot=0)\n",
        "plt.xlabel(\"Survived\", labelpad=14)\n",
        "plt.ylabel(\"Count of Survived\", labelpad=14)\n",
        "plt.title(\"Count of Survived\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GxQZEfTtzm0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking correlation in our data set\n",
        "corrmatrix = df.corr()\n",
        "plt.subplots(figsize=(20,8))\n",
        "sns.heatmap(corrmatrix, annot = True,vmax = 0.9, vmin = -0.3, linewidths = 0.2, cmap=\"BuPu\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gcLrqZgmznB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "i7-LRwG40nED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting only non categorical data\n",
        "df1 = df[['Age','Pclass','SibSp','Parch','Fare','Survived']]\n",
        "df1"
      ],
      "metadata": {
        "id": "KO2Z_IU3MviX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assign independent and dependent variables\n",
        "y = df1['Survived']\n",
        "X = df1.drop('Survived', axis=1)\n"
      ],
      "metadata": {
        "id": "B4-q4ogIMvwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistical summary of X\n",
        "X.describe()"
      ],
      "metadata": {
        "id": "nMuHfErJMwLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are three Scaling method in the preprocessing\n",
        "\n",
        "1. Standard Scaler\n",
        "2. Min Max Scaler\n",
        "3. Normalization\n",
        "\n",
        "As per the question we can use with Min Max Scaler\n",
        "Minmax scaling is done to convert all our values in the range 0-1.\n",
        "\n"
      ],
      "metadata": {
        "id": "JzVXz0fOcSEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's import preprocessing from sklearn library and do minmax scaling\n",
        "from sklearn import preprocessing\n",
        "min_max = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
        "X = min_max.fit_transform(X)\n",
        "X = pd.DataFrame(X)"
      ],
      "metadata": {
        "id": "DzJq2N1PMwN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "id": "NiUPXckYMwQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.describe()"
      ],
      "metadata": {
        "id": "pYWIsBmsMwR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can see that our data is scaled with MinMaxScaler"
      ],
      "metadata": {
        "id": "DDTFkj3vcrA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train and test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
        "\n",
        "# Linear model\n",
        "from sklearn import linear_model\n",
        "lr = linear_model.LinearRegression()\n",
        "model = lr.fit(X_train,y_train)\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# MSE and R Squared\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "print(\"MSE is: \", mean_squared_error(y_test, predictions))\n",
        "print(\"R Squared is: \", r2_score(y_test, predictions))"
      ],
      "metadata": {
        "id": "ejU7Fz3bMwUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MSE value is low which means scaling has no adverse effect on our model. And R Squared is very low which means our model is not accurate."
      ],
      "metadata": {
        "id": "_ZW14JdodAgq"
      }
    }
  ]
}